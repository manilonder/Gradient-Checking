# Gradient-Checking
Backpropagation computes the gradients  âˆ‚ğ½/âˆ‚ğœƒ , where  ğœƒ  denotes the parameters of the model.  ğ½  is computed using forward propagation and your loss function.
Because forward propagation is relatively easy to implement, you're confident you got that right, and so you're almost 100% sure that you're computing the cost  ğ½  correctly. Thus, you can use your code for computing  ğ½  to verify the code for computing  âˆ‚ğ½/âˆ‚ğœƒ .
